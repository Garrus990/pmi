{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmi_case.utils import *\n",
    "import pandas as pd\n",
    "import plotnine as pln\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation (initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('../data/dataset.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.str.upper()\n",
    "# set correct type for datetime columns\n",
    "data['PRODUCTION END DATE.DAY'] = pd.to_datetime(data['PRODUCTION END DATE.DAY'], infer_datetime_format=True)\n",
    "data['RELEASE DATE'] = pd.to_datetime(data['RELEASE DATE'], infer_datetime_format=True)\n",
    "# BATCHID_ALT was wrogly read as a numeric column\n",
    "data['BATCHID_ALT'] = data['BATCHID_ALT'].astype(np.str)\n",
    "del data['BATCH SIZE UNIT']\n",
    "data = data.apply(lambda x: x.str.strip() if x.dtype == 'object' else x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that consist of one value are uninformative and are of no use in the modeling part, so we drop them in advance, not to spend unnecesarily time on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = drop_columns_with_single_value(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check what are the types of the columns to have an info about their (most probable, since it is a heuristic) type and whether it will be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types = data.apply(validate_column, axis=0, n_not_null=N_NOT_NULL, \n",
    "                          min_observations_in_class=MIN_OBSERVATIONS_IN_CLASS).\\\n",
    "    T.set_axis(['analysable', 'column_type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case we drop all columns that have too many NAs - that many, that it is not possible to sensibly impute the missing values\n",
    "data = data.loc[:, ~column_types['column_type'].str.contains('Not enough values')]\n",
    "column_types = column_types.loc[~column_types['column_type'].str.contains('Not enough values')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we plot all of the remaining columns (of appropriate types) with a graph type that is suitable to analyse the kind of data we have assumed for that variable. It will help us what can be enhanced on top of what we have previously implemented. Beware - it will result in a __long__ output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore')\n",
    "for i, (col_name, vals) in enumerate(data.iteritems()):\n",
    "    col_name_clean = col_name.replace('_', ' ').upper()\n",
    "    col_type = column_types.loc[col_name]['column_type']\n",
    "\n",
    "    if col_type in ('binary', 'multiclass', 'Not enough classes (0 or 1).'):\n",
    "        p1 = (pln.ggplot(data, pln.aes(x=col_name)) +\n",
    "              pln.geom_bar(fill='#ed4c9a') +\n",
    "              pln.geom_text(pln.aes(y='stat(count)', label='stat(count)'), stat='count', va='bottom') +\n",
    "              pln.labs(title=col_name_clean, x='', y='') +\n",
    "              pln.theme_bw()\n",
    "        )\n",
    "        if data[col_name].unique().shape[0] > 10:\n",
    "            p1 = p1 + pln.theme(axis_text_x=pln.element_text(angle=90))\n",
    "    elif col_type == 'regression':\n",
    "        p1 = (pln.ggplot(data, pln.aes(x=col_name)) +\n",
    "              pln.geom_density(color='#ed4c9a') +\n",
    "              pln.labs(title=f'Estimated density for {col_name_clean}', x=col_name_clean, y='') +\n",
    "              pln.theme_bw()\n",
    "              )\n",
    "    else:\n",
    "        continue\n",
    "    display(HTML(f'<h1><center>{col_name_clean}</center></h1>'))\n",
    "    print(p1)\n",
    "\n",
    "    display(HTML('<hr>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aftermath of the above round of plotting:\n",
    "- drop columns where there is one, domainating value and other values occur significantly less frequently (for example: `HLAT LENGTH TARGET`);\n",
    "- log `TARGET` (it has a heavy right tail);\n",
    "- some variables have values that greatly deviate from the usual observations - they are likely to be erroneous entries and needs to be taken care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with one domainating value - it will help with smooth cross validation\n",
    "data = data.loc[:, ~column_types['column_type'].str.contains('Not enough')]\n",
    "# log TARGET\n",
    "data['TARGET'] = np.log(data['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with erroneous entries\n",
    "cols = ['PSD', 'DLBL DIAM AVG', 'DLBL DIAM LL', 'DLBL DIAM UL', 'DLBL LENGTH AVG',\n",
    "        'DLBL TP PLUG BACK LENGTH AVG', 'DLBL TP BACK LENGTH STD', 'DLBL RTD AVG',\n",
    "        'DLBL RTD LL', 'DLBL RTD UL', 'DLBL RTD TARGET', 'TLR DIAM AVG',\n",
    "        'TLR DIAM STD', 'LL TLR DIAM', 'HLAT DIAM AVG', 'HLAT DIAM STD',\n",
    "        'LL HLAT DIAM', 'UL HLAT DIAM', 'HLAT FRONT DIAM', 'LL HLAT FRONT DIAM',\n",
    "        'UL HLAT FRONT DIAM', 'HLAT LENGTH', 'HLAT LENGTH STD', 'LL HLAT LENGTH',\n",
    "        'UL HLAT LENGTH', 'PLLA DIAMETER', 'PLLA DIAM STD', 'PLLA LENGTH',\n",
    "        'PLLA OVALITY', 'PLLA OVAL STD',\n",
    "        ]\n",
    "\n",
    "data[cols] = data[cols].apply(drop_erroneous_values, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also happens, that 5 objects in the sample have atypical reads for the five measurements: `LL PLLA RTD`, `UL PLLA RTD`, `PLLA RTD TARGET`, `PLLA RTD STD`, `PLLA LENGTH STD`. For these 5 objects all of the values for the given variables are zeros, whereas other objects have no-zero values. We therefore decide to drop these objects as there is probably more persistent fault with these ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[~(data[['LL PLLA RTD', 'UL PLLA RTD', 'PLLA RTD TARGET', 'PLLA RTD STD', 'PLLA LENGTH STD']] == 0).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-draw the data once again, after our laborous corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore')\n",
    "for i, (col_name, vals) in enumerate(data.iteritems()):\n",
    "    col_name_clean = col_name.replace('_', ' ').upper()\n",
    "    col_type = column_types.loc[col_name]['column_type']\n",
    "\n",
    "    if col_type in ('binary', 'multiclass', 'Not enough classes (0 or 1).'):\n",
    "        p1 = (pln.ggplot(data, pln.aes(x=col_name)) +\n",
    "              pln.geom_bar(fill='#ed4c9a') +\n",
    "              pln.geom_text(pln.aes(y='stat(count)', label='stat(count)'), stat='count', va='bottom') +\n",
    "              pln.labs(title=col_name_clean, x='', y='') +\n",
    "              pln.theme_bw()\n",
    "              )\n",
    "        if data[col_name].unique().shape[0] > 10:\n",
    "            p1 = p1 + pln.theme(axis_text_x=pln.element_text(angle=90))\n",
    "    elif col_type == 'regression':\n",
    "        p1 = (pln.ggplot(data, pln.aes(x=col_name)) +\n",
    "              pln.geom_density(color='#ed4c9a') +\n",
    "              pln.labs(title=f'Estimated density for {col_name_clean}', x=col_name_clean, y='') +\n",
    "              pln.theme_bw()\n",
    "              )\n",
    "    else:\n",
    "        continue\n",
    "    display(HTML(f'<h1><center>{col_name_clean}</center></h1>'))\n",
    "    print(p1)\n",
    "\n",
    "    display(HTML('<hr>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clearing process that we applied above is not perfect, but it helped us to eliminate some obvious errors / wrong measurements. The data looks overall better and is more suitable for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we cannot use `MACHINE ID` or `MATERIAL GROUP` as training features, even though they look promissing and could be impactful in the analysis. It is due to multiple infrequent values that we do not want to remove since we cannot impute these columns (infrequent groups will be problematic in cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fields = data.loc[:, ~data.columns.isin(\n",
    "    ['BATCHID', 'BATCHID_ALT', 'GLOBAL REVISION NUMBER', 'PRODUCTION END DATE.DAY',\n",
    "     'RELEASE DATE', 'BATCH STATUS', 'MACHINE ID', 'PRODUCT CODE', 'MATERIAL GROUP',\n",
    "     'TARGET', 'TARGET UPPER SPECIFICATION VALUE', 'TARGET SPECIFICATION VALUE',\n",
    "     'TARGET EXPONENT VALUES', 'TARGET INVERSED VALUES'])].columns\n",
    "y_field = 'TARGET'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see below - there are columns with missing values, which would be cumbersome during the training, since the data cannot have any such entries. We will impute them using one of the moderately sophisticated methods - random-forest-based imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[X_fields].isna().sum(axis=0).plot.hist()\n",
    "plt.title('Number of missing values per feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from missingpy import MissForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = MissForest(max_iter=15, n_estimators=200, n_jobs=4, random_state=250)\n",
    "data[X_fields] = imputer.fit_transform(data[X_fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Number of missing values per feature')\n",
    "plt.show()\n",
    "data[X_fields].isna().sum(axis=0).plot.hist()  # no missing data anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the points that we are dealing with may be multi-dimensional outliers and we wouldn't like to work with them as they are corrupting most of the algorithms that are used for classification / regression. We will get rid of them using another random-forest-based algorithm called Isolation Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assume that ~2% of all observations are outliers - should be discussed with a subject expert\n",
    "isolation_forest = IsolationForest(n_estimators=200, contamination=0.02, n_jobs=4, random_state=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_outlier = -1*isolation_forest.fit_predict(data[X_fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(is_outlier, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out, that the algorithm considers 5 points to be outliers. We remove all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[~(is_outlier == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA CONTINUED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when all the missing values are gone and so are the outliers, we can draw correlation matrix between the numerical values and `TARGET`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_columns = X_fields.union(['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='white')\n",
    "correlogram = sns.clustermap(data[plot_columns].corr(), center=0, cmap='vlag',\n",
    "                             figsize=(25, 25), dendrogram_ratio=0.15,\n",
    "                             row_colors=['#00c434' if col == 'TARGET' else '#c4c41b' for col in plot_columns])\n",
    "correlogram.ax_row_dendrogram.remove()\n",
    "plt.title('Correlogram for independent variables (golden bars on the left) and TARGET (green bar)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TARGET variable seems not to be significantly correlated with many features. We can also observe clusters of tightly correlated variables. One such prominent case are `HLAT` variables. Below, we show the correlation matrix to examine further to what extent the variables are correlated and whether this can hinder the down-the-line analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter(regex='HLAT', axis=1).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is, indeed, correlation between features of more than 0.9999 which means that these variables offer almost the same information. This entails redundancy between them. We will remove columns that have this level of co-linearity. The same situation may be present with other _categories_ of variables (like `TLR` or `PLLA`) and we will address these as well, in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.999)]\n",
    "print(f'{len(to_drop)} columns will be removed due to excessively high correlation: {to_drop}')\n",
    "data = data.drop(columns=to_drop)\n",
    "X_fields = X_fields.difference(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the preprocessing of the data is finalized, we store the results for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_store = {'data': data, 'X_fields': X_fields, 'y_field': y_field}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(data_to_store, open('../data/processed_data/data_after_preprocessing.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if needed - we can read the preprocessed data from the file\n",
    "stored_data = pkl.load(open('../data/processed_data/data_after_preprocessing.pkl', 'rb'))\n",
    "data, X_fields, y_field = stored_data['data'], stored_data['X_fields'], stored_data['y_field']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also plot whether target is dependent on the weekday of the production end / release day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PRODUCTION_END_DATE_WEEKDAY'] = data['PRODUCTION END DATE.DAY'].dt.weekday.replace(WEEKDAY_TRANSLATION_DAY)\n",
    "data['RELEASE_DATE_WEEKDAY'] = data['RELEASE DATE'].dt.weekday.replace(WEEKDAY_TRANSLATION_DAY)\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "data['PRODUCTION_END_DATE_WEEKDAY'] = pd.Categorical(data['PRODUCTION_END_DATE_WEEKDAY'], \n",
    "                                                     categories=weekdays, ordered=True).remove_unused_categories()\n",
    "data['RELEASE_DATE_WEEKDAY'] = pd.Categorical(data['RELEASE_DATE_WEEKDAY'], \n",
    "                                              categories=weekdays, ordered=True).remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA for weekday of production's end\n",
    "_, p_val = stats.f_oneway(*[vals.values for _, vals in data.groupby('PRODUCTION_END_DATE_WEEKDAY')['TARGET']])\n",
    "\n",
    "(pln.ggplot(data, pln.aes(x='PRODUCTION_END_DATE_WEEKDAY', y='TARGET')) +\n",
    " pln.geom_boxplot() +\n",
    " pln.labs(title=\"TARGET with respect to the weekday of the production's end\\n\"\n",
    "                f\"One-way ANOVA p-value: {np.round(p_val, 3)}\", x='') +\n",
    " pln.theme_bw() +\n",
    " pln.theme(figure_size=(8, 6))\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA for weekday of release date\n",
    "_, p_val = stats.f_oneway(*[vals.values for _, vals in data.dropna().groupby('RELEASE_DATE_WEEKDAY')['TARGET']])\n",
    "\n",
    "(pln.ggplot(data, pln.aes(x='RELEASE_DATE_WEEKDAY', y='TARGET')) +\n",
    " pln.geom_boxplot() +\n",
    " pln.labs(title=\"TARGET with respect to the weekday of the release day\\n\"\n",
    "                f\"One-way ANOVA p-value: {np.round(p_val, 3)}\", x='') +\n",
    " pln.theme_bw() +\n",
    " pln.theme(figure_size=(8, 6))\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eyeballing, there doesn't seem to be any significant difference with respect to weekday of both: production's end and release date. This is further confirmed by one-way ANOVA statistical test which, in both cases, we do not reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also analyse production's end's and release date's month so that we have wider, temporal, perspective on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PRODUCTION END MONTH'] = pd.Categorical(data['PRODUCTION END DATE.DAY'].dt.month, ordered=True)\n",
    "data['RELEASE DATE MONTH'] = pd.Categorical(data['RELEASE DATE'].dt.month, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA for weekday of release date\n",
    "_, p_val = stats.f_oneway(*[vals.values for _, vals in data.dropna().groupby('PRODUCTION END MONTH')['TARGET']])\n",
    "\n",
    "(pln.ggplot(data, pln.aes(x='PRODUCTION END MONTH', y='TARGET')) +\n",
    " pln.geom_boxplot() +\n",
    " pln.labs(title=\"TARGET with respect to the month of the production's end\\n\"\n",
    "                f\"One-way ANOVA p-value: {np.round(p_val, 3)}\", x='') +\n",
    " pln.theme_bw() +\n",
    " pln.theme(figure_size=(8, 6))\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA for weekday of release date\n",
    "_, p_val = stats.f_oneway(*[vals.values for _, vals in data.dropna().groupby('RELEASE DATE MONTH')['TARGET']])\n",
    "\n",
    "(pln.ggplot(data, pln.aes(x='RELEASE DATE MONTH', y='TARGET')) +\n",
    " pln.geom_boxplot() +\n",
    " pln.labs(title=\"TARGET with respect to the month of the release\\n\"\n",
    "                f\"One-way ANOVA p-value: {np.round(p_val, 3)}\", x='') +\n",
    " pln.theme_bw() +\n",
    " pln.theme(figure_size=(8, 6))\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in these cases, there is no indication on significance of the temporal features, so we disregard them in the modeling part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling `TARGET`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually - we are predicting logarithm of `TARGET`, since the values were transformed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold, KFold, train_test_split, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "random_state = np.random.RandomState(1847)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_cv = KFold(5)\n",
    "k_fold = RepeatedKFold(n_splits=5, n_repeats=5)\n",
    "\n",
    "lr_model = LinearRegression(n_jobs=2)\n",
    "lr_model = make_pipeline(StandardScaler(), lr_model)  # we standardize values so that coefficients of LR are directly comparable\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=201, n_jobs=2, random_state=random_state)\n",
    "rf_grid = GridSearchCV(rf_model, scoring='r2', param_grid={'max_depth': [3, 5, 7]}, n_jobs=3, cv=inner_cv, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size=28 so that we are left with 200 observations in the training set\n",
    "train_set, holdout_set = train_test_split(data, test_size=28, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run cross-validation to initially verify models' performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[X_fields].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv_res = cross_validate(lr_model, X=train_set[X_fields].values, y=train_set[y_field].values, scoring='r2', cv=k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 for linear regression in 5x repeated 5-fold CV: {np.round(lr_cv_res['test_score'].mean(), 3)}\"\n",
    "      f\" +/- {np.round(lr_cv_res['test_score'].std(), 3)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something is terribly wrong with logistic regression - we cannot trust this model in our predictions. Finding out the reason would require some drilling down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv_res = cross_validate(rf_grid, X=train_set[X_fields].values, y=train_set[y_field].values, scoring='r2', cv=k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 for random forest in 5x repeated 5-fold CV: {np.round(rf_cv_res['test_score'].mean(), 3)}\"\n",
    "      f\"+/- {np.round(rf_cv_res['test_score'].std(), 3)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the random forest model on the whole training data and test on holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(train_set[X_fields], train_set[y_field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 on the holdout set for random forest training on the whole training set: \"\n",
    "      f\"{np.round(r2_score(holdout_set[y_field], rf_model.predict(holdout_set[X_fields])), 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pln.ggplot(data=pd.DataFrame({'prediction': rf_model.predict(holdout_set[X_fields]),\n",
    "                               'truth': holdout_set[y_field]}), mapping=pln.aes(x='truth', y='prediction')) +\n",
    " pln.geom_point(size=3, fill='blue') +\n",
    " pln.labs(title='Predictions from random forest on holdout set vs. true values', x='Truth', y='Prediction') +\n",
    " pln.theme_bw()\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions from the random forest are much more accurate for the holdout set than in cross-validation, which is worrying and should be closlier investigated. Such a discrepancy may be attributed to a lucky split into training and holdout set, so it is inevitable to implement a more thorough validation procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance from random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_df = pd.DataFrame({'importance': rf_model.feature_importances_,\n",
    "                      'feature': train_set[X_fields].columns})\n",
    "\n",
    "(pln.ggplot(pln.aes(x='feature', y='importance'), data=fi_df.sort_values('importance', ascending=False).iloc[:10]) +\n",
    " pln.geom_segment(pln.aes(x='feature', xend='feature', y=0, yend='importance'), color=\"skyblue\") +\n",
    " pln.geom_point(color=\"blue\", size=4, alpha=0.8) +\n",
    " pln.labs(x='Feature', y='Importance [Gini]', \n",
    "          title='Importance of the features for the random forest model\\n (trained on 200 observations)') +\n",
    " pln.coord_flip() +\n",
    " pln.theme_bw() +\n",
    " pln.theme(\n",
    "    panel_grid_major_y=pln.element_blank(),\n",
    "    panel_border=pln.element_blank(),\n",
    "    axis_ticks_major_y=pln.element_blank()\n",
    " )\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(holdout_set[X_fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanations for the 2nd observation in the holdout set which has a (relatively) big prediction\n",
    "shap.force_plot(explainer.expected_value, shap_values[2, :], np.round(holdout_set[X_fields].iloc[2, :], 3), matplotlib=True, text_rotation=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanations for the last observation in the holdout set which has a strongly negative prediction\n",
    "shap.force_plot(explainer.expected_value, shap_values[-1, :], np.round(holdout_set[X_fields].iloc[-1, :], 3), matplotlib=True, text_rotation=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explantaions for the whole holdout set\n",
    "shap.force_plot(explainer.expected_value, shap_values, holdout_set[X_fields])  # doesn't work in jupyterlab :("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
